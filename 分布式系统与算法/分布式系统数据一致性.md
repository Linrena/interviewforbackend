## 分布式系统

分布式系统是一组[电脑](https://zh.wikipedia.org/wiki/電子計算機)，透过[网络](https://zh.wikipedia.org/wiki/計算機網絡)相互连接[传递消息](https://zh.wikipedia.org/wiki/訊息傳遞_(軟體))与通信后并协调它们的行为而形成的系统。组件之间彼此进行交互以实现一个共同的目标。把需要进行大量计算的工程数据分割成小块，由多台[计算机](https://zh.wikipedia.org/wiki/電子計算機)分别计算，再上传运算结果后，将结果统一合并得出数据结论。

[A distributed system is a computing environment in which various components are spread across multiple computers (or other computing devices) on a network. These devices split up the work, coordinating their efforts to complete the job more efficiently than if a single device had been responsible for the task.](https://www.splunk.com/en_us/data-insider/what-are-distributed-systems.html) [From Splunk]

## CAP 理论

分布式系统 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和 Partition Tolerance(分区容忍性) 这三个特性的，最多只能同时满足其中两个。

但在实际分布式系统中，由于网络环境是不可信的，所以分区容忍性几乎是必选的，设计者基本就是在一致性和可用性之间做选择，当然大部分情况下，大家都会选择牺牲一部分的一致性来保证可用性（可用性较差的系统非常影响用户体验的，但是对另一些场景，比如支付场景，强一致性是必须的）。

- **一致性：**对于客户端的每次读操作，要么读到的是最新的数据，要么读取失败。换句话说，一致性是站在分布式系统的角度，对访问本系统的客户端的一种承诺：要么我给您返回一个错误，要么我给你返回绝对一致的最新数据，不难看出，其强调的是数据正确。
- **可用性：**任何客户端的请求都能得到响应数据，不会出现响应错误。换句话说，可用性是站在分布式系统的角度，对访问本系统的客户的另一种承诺：我一定会给您返回数据，不会给你返回错误，但不保证数据最新，强调的是不出错。
- **分区容忍性：**由于分布式系统通过网络进行通信，网络是不可靠的。当任意数量的消息丢失或延迟到达时，系统仍会继续提供服务，不会挂掉。换句话说，分区容忍性是站在分布式系统的角度，对访问本系统的客户端的再一种承诺：我会一直运行，不管我的内部出现何种数据同步问题，强调的是不挂掉。

![img](../shortcuts/(null)-20220305163055943.(null))

很多 NoSQL 数据库都是 AP 型的，其中最常见的 MongoDB 是 CP 型的。

![img](../shortcuts/(null)-20220305163055954.(null))

2PC 和 3PC 是 CA 型的，Paxos 和 Raft 是 CP 型的，而 MVCC 是 AP 型的。

## 一致性

一致性（Consistency）是指多副本（Replications）问题中的数据一致性（由多数据拷贝带来的问题而不是多并发读写）。关于分布式系统的一致性模型有以下几种：

- **强一致性：**当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值，直到这个数据被其他数据更新为止。但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。

- **弱一致性：**系统并不保证进程或者线程的访问都会返回最新更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。甚至不能保证可以访问到。

- **最终一致性**：最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一个时间，在这个时间之后可以保证这一点（就是在一段时间后，节点间的数据会最终达到一致状态），而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为「不一致窗口」。不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。

# 协议与算法

## 分布式事务与共识

分布式事务：二阶段、三阶段

共识算法：Raft、Paxos

关系：

像两阶段和三阶段提交协议都是用来支撑分布式事务一致性的，而 Paxos、Raft 以及 ZAB 是用来解决分布式系统数据多副本之间的一致性，即达成“共识”，我们要将共识和事务一致性分开看待，这两者并不是一回事。简单来说：

- **分布式事务一致性**会因为引入协调者而出现单点可用性问题
- 为了解决可用性问题，分布式事务的节点需要在协调者故障时就新协调者选取达成**共识**
- **解决共识问题**等价于实现一个**线性一致**存储（线性一致，linearability，指**多副本的系统能够对外表现地像只有单个副本一样且**系统保证从任何副本读取到的值都是最新的，即强一致性）
- **解决线性一致问题就要**实现**全序广播（total order boardcast）而Paxos/Raft 实现了全序广播**

但从共性上来讲，上面这些协议与算法都是用来解决分布式系统数据一致性的相关问题。

## 2PC 两阶段提交

二阶段提交协议（Two-phase Commit，即 2PC）是常用的分布式事务解决方案，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现 ACID 的原子性（A）。在数据一致性中，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来保证数据的强一致性。

2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为**协调者**的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

### 2PC 过程

关于两阶段提交的过程如下图所示：

![img](../shortcuts/(null)-20220305163056013.(null))

两阶段提交过程

顾名思义，2PC 分为两个过程：

1. 表决阶段：此时 Coordinator （协调者）向所有的参与者发送一个 vote request，参与者在收到这请求后，如果准备好了就会向 Coordinator 发送一个 `VOTE_COMMIT` 消息作为回应，告知 Coordinator 自己已经做好了准备，否则会返回一个 `VOTE_ABORT` 消息；
2. 提交阶段：Coordinator 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么 Coordinator 就会发送 `GLOBAL_COMMIT` 消息，否则发送 `GLOBAL_ABORT` 消息；对于参与者而言，如果收到 `GLOBAL_COMMIT` 消息，就会提交本地事务，否则就会取消本地事务（保证了数据强一致性）。

### 2PC 一致性问题

这里先讨论一下，2PC 是否可以在任何情况下都可以解决一致性问题，在实际的网络生产中，各种情况都有可能发生，这里我们先从理论上分析各种意外情况。

2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。

第一种情况，Coordinator 和参与者两者挂其一：

无法复制加载中的内容

同时挂，分为几种类型来讨论：

无法复制加载中的内容

### 2PC 优缺点

简单总结一下 2PC 的优缺点：

- 优点：原理简洁清晰、实现方便；

- 缺点：同步阻塞、单点问题、小概率数据不强一致。

前两个缺点，在实际应用中，对 2PC 做相应的改造：

1. 同步阻塞：Coordinator 等待所有参与者表决的过程中是同步阻塞的，在实际的应用中，这可能会导致参与者长阻塞问题，这个问题是通过超时判断机制来解决的（防止某个参与者挂了而长时间阻塞），但并不能完全解决同步阻塞问题；
2. Coordinator 单点问题：实际生产应用中，Coordinator 都会有相应的备选节点；





## 3PC 三阶段提交

三阶段提交协议（Three-Phase Commit， 3PC）最关键要解决的就是两阶段提交协议过程中的同步阻塞问题，所以 3PC 在 2PC 中又添加一个阶段，这样三阶段提交就有：CanCommit、PreCommit 和 DoCommit 三个阶段。

3PC 通过以下手段降低了阻塞：

- 参与者返回 CanCommit 请求的响应后，等待第二阶段指令，若等待超时，则自动 abort，减少阻塞；
- 参与者返回 PreCommit 请求的响应后，等待第三阶段指令，若等待超时，则自动 commit 事务，也减少了阻塞；

### 3PC 过程

三阶段提交协议的过程如下图所示：

![img](../shortcuts/(null))

3PC 的详细过程：

#### 阶段一 CanCommit（询问）

1. 事务询问：Coordinator 向各参与者发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；
2. 参与者向 Coordinator 反馈询问的响应：参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No。

#### 阶段二 PreCommit（执行）

**执行事务预提交**：如果 Coordinator 接收到各参与者反馈都是 Yes，那么执行事务预提交：

1. 发送预提交请求：Coordinator 向各参与者发送 preCommit 请求，并进入 prepared 阶段；
2. 事务预提交：参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；
3. 各参与者向 Coordinator 反馈事务执行的响应：如果参与者成功执行了事务操作，那么反馈给协调者 ACK 响应，同时等待最终指令进行 commit 或者 abort；

**中断事务**：如果任何一个参与者向 Coordinator 反馈了 No 响应，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务。

1. 发送中断请求：Coordinator 向所有参与者发送 abort 请求；
2. 中断事务：无论是收到来自 Coordinator 的 abort 请求，还是等待超时，参与者都中断事务。

#### 阶段三 DoCommit（提交）

**执行提交**

1. 发送提交请求：假设 Coordinator 正常工作，接收到了所有参与者的 ack 响应，那么它将从预提交阶段进入提交状态，并向所有参与者发送 doCommit 请求；
2. 事务提交：参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；
3. 反馈事务提交结果：参与者完成事务提交后，向 Coordinator 发送 ACK 信息；
4. 完成事务：Coordinator 接收到所有参与者 ack 信息，完成事务。

**中断事务**：假设 Coordinator 正常工作，并且有任一参与者反馈 No，或者在等待超时后无法接收所有参与者的反馈，都会中断事务

1. 发送中断请求：Coordinator 向所有参与者节点发送 abort 请求；
2. 事务回滚：参与者接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资源；
3. 反馈事务回滚结果：参与者在完成事务回滚之后，向 Coordinator 发送 ack 信息；
4. 中断事务：Coordinator 接收到所有参与者反馈的 ack 信息后，中断事务。

### 3PC 分析

3PC 虽然减少了同步阻塞，但依然存在其他问题，比如网络分区问题。在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit（CA without P）。

而且由于3PC 的设计过于复杂，在解决2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。





## Paxos

从「拜占庭将军问题」到「Paxos 小岛的故事」诞生了 Paxos 算法。

`Paxos` 算法是基于**消息传递且具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一，**其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。

在 `Paxos` 中主要有三个角色，分别为 `Proposer提案者`、`Acceptor表决者`、`Learner学习者`。`Paxos` 算法和 `2PC` 一样，也有两个阶段，分别为 `Prepare` 和 `accept` 阶段。

在具体的实现中，一个进程可能同时充当多种角色。比如一个进程可能既是 Proposer 又是 Acceptor 又是 Learner。Proposer 负责提出提案，Acceptor 负责对提案作出裁决（accept 与否），learner 负责学习提案结果。

这里有一个很重要的概念叫「**提案**」（Proposal）。最终要达成一致的 value 就在提案里。只要 Proposer 发的提案被半数以上的 Acceptor 接受，Proposer 就认为该提案里的 value 被选定了。Acceptor 告诉 Learner 哪个 value 被选定，Learner 就认为那个 value 被选定。

#### 阶段一：prepare 阶段

1. `Proposer` 负责提出 `proposal`，每个提案者在提出提案时都会首先获取到一个具有全局唯一性的、递增的提案编号 N，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。
2. 如果一个 Acceptor 收到一个编号为 N 的 Prepare 请求，如果小于它已经响应过的请求，则不回应或回复 error 以表拒绝。若 N 大于该 Acceptor 已经响应过的所有 Prepare 请求的编号（maxN），那么它就会将它在这之前已经批准过的编号最大的提案（如果有的话，如果没有 accept 过提案的话返回{pok，null，null}）作为响应反馈给 Proposer，同时该 Acceptor 承诺不再接受任何编号小于 N 的提案

#### 阶段二：accept 阶段

1. 如果一个 Proposer 收到半数以上 Acceptor 对其发出的编号为 N 的 Prepare 请求的响应，那么它就会发送一个针对 [N,V] 提案的 Accept 请求半数以上的 Acceptor。注意：V 就是收到的 Acceptor 响应中编号最大的提案的 value，如果响应中不包含任何提案，那么 V 就由 Proposer 自己决定
2. 如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor 没有对编号大于 N 的 Prepare 请求做出过响应，它就通过该提案。如果 N 小于 Acceptor 以及相应的 prepare 请求，则拒绝，不回应或回复 error（当 proposer 没有收到过半的回应，那么他会重新进入第一阶段，递增提案号，重新提出 prepare 请求）
3. 最后是 Learner 获取通过的提案（有多种方式）

![img](../shortcuts/(null)-20220305163056105.(null))

#### 死循环问题

其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。

比如说，提案者 P1 提出一个方案 M1，完成了 `Prepare` 阶段的工作，这个时候 `acceptor` 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 `Prepare` 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 `acceptor` 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 `Prepare` 阶段，然后 `acceptor` ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 `Prepare` 阶段。。。

就这样无休无止的永远提案下去，这就是 `paxos` 算法的死循环问题。

**Solution**: 可以选择一个 Proposer 作为主 Proposer，并约定只有主Proposer才可以提出提案。因此，只要主Proposer可以与过半的Acceptor保持通信，那么但凡主Proposer提出的编号更高的提案，均会被批准。





## ZAB

ZAB（Zookeeper Atomic Broadcast）协议是为分布式协调服务 Zookeeper 专门设计的一种支持**崩溃恢复的****原子****广播协议**。

在 Zookeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种**主备模式**的系统架构来保持集群中各副本之间**数据的一致性**。

尽管 ZAB 不是 Paxos 的实现，但是 ZAB 也参考了一些 Paxos 设计思想，比如：

- **leader 向 followers 提出提案(proposal)**

- **leader 需要在达到法定数量(半数以上)的 followers 确认之后才会进行 commit**

- **每一个 proposal 都有一个纪元(epoch)号**

#### `**ZAB**` 中的三个角色

`ZAB` 中有三个主要的角色，`Leader领导者`、`Follower跟随者`、`Observer观察者` 。

- `**Leader**` **：集群中唯一的写请求处理者** ，能够发起投票（投票也是为了进行写请求）。

- `**Follower**`**：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。**

- `**Observer**` **：就是没有选举权和被选举权的 Follower 。**

#### 消息广播模式

![img](../shortcuts/(null)-20220305163055942.(null))

ZAB 广播

1. Leader 从客户端收到一个提案请求（如果是集群中其他机器接收到客户端的提案请求即 Proposal，会直接转发给 Leader 服务器）
2. Leader 服务器生成一个对应的提案 Proposal，并为这个提案生成一个全局递增的唯一的 ZXID（通过其 ZXID 来进行排序保证顺序性）
3. Leader 将这个提案发送给所有的 Followers 节点
4. Follower 节点将收到的提案请求加入到历史队列（Leader 会为每个 Follower 分配一个单独的队列先进先出，顺序保证消息的因果关系）中，并发送 ack 给 Leader
5. 当 Leader 收到超过半数 Follower 的 ack 消息，Leader 会广播一个 commit 消息
6. 当 Follower 收到 commit 请求时，会判断该提案的 ZXID 是不是历史队列中的最小 ZXID，如果是则提交，如果不是则等待比它更小的提案的 commit

![img](../shortcuts/(null)-20220305163056072.(null))

zab commit 流程

#### 崩溃恢复模式

单点故障问题：一旦 Leader 服务器挂掉或者由于网络原因导致与半数的 Follower 的服务器失去联系，那么就会进入崩溃恢复模式。整个恢复过程结束后需要选举出一个新的 Leader 服务器。

恢复模式大致可以分为四个阶段：**选举、发现、同步、广播**

1. 当 leader 崩溃后，集群进入选举阶段，开始选举出潜在的新 leader (一般为集群中拥有最大 ZXID 的节点)
2. 进入发现阶段，follower 与潜在的新 leader 进行沟通，如果发现超过法定人数的 follower 同意，则潜在的新 leader 将 epoch 加 1，进入新的纪元。新的 leader 产生
3. 集群间进行数据同步，保证集群中各个节点的提案一致
4. 集群恢复到广播模式，开始接受客户端的写请求

##### 两个特性

根据 ZAB 消息广播的过程可知，如果一个提案 Proposal 在一台机器上被处理成功，那么就应该在所有的机器上处理成功，哪怕机器出现故障。所以在崩溃恢复过程结束后，为了保证新选举出来的 Leader 服务器能正常工作，需要保证 2 个基本特性：

- **确保那些已经在 Leader 服务器上提交的提案最终被所有的服务器都提交**

- **确保丢弃那些只在 Leader 服务上被提出的提案**

首先看第一个特性：确保那些已经在 Leader 服务器上提交的提案最终被所有的服务器都提交。试想这么个场景：Leader 服务器在收到超半数的 ACK 返回响应后，本应该广播 commit 消息，但这时候 Leader 服务器挂掉了（Leader 服务器已经提交了提案），这时候就会导致 Follower 服务器和 Leader 服务器数据不一致的情况。ZAB 协议就须确保这种况下，所有的 Follower 服务器也都成功提交提案。

第二个特性：确保丢弃那些只在 Leader 服务上被提出的提案。试想这么个场景：Leader 服务器在生成 Proposal 后就挂掉了，其他的服务器都没收到该 Proposal。于是，当该机器再次加入集群中的时候，需要确保丢弃该 Proposal。

所以上面两个特性总结就是下面两句话：

- **提交已经只被 Leader 提交的提案**

- **丢弃已经被跳过的提案**

基于这两个特性，如果让新选举出来的 Leader 具有最大的 ZXID 的提案 Proposal，那么就可以保证该 Leader 一定具有所有已提交的提案。更为重要的是，如果让具有最高 ZXID 的提案 Proposal 的机器来成为 Leader，就可以省去 Leader 服务器检查 Proposal 的提交和丢弃工作这一步操作了。

##### 数据同步

在完成 Leader 选举之后，在正式开始工作（即接收客户端的提案请求，然后提出新的提案）之前，Leader 服务器首先会确保提案日志中的所有 Proposal 是否都已经被集群中过半的机器提交了，即是否完成数据同步。

对于那些没有被 Follower 服务器提交的提案，Leader 会为每个 Follower 服务器准备一个队列，并将那些没有被各 Follower 服务器同步的提案以 Proposal 消息的形式逐个发送给 Follower 服务器，并在每一个 Proposal 消息后面紧接着再发送一个 commit 消息，以表示该提案已被提交。等到 Follower 服务器将所有未同步的提案 Proposal 都从 Leader 服务器上同步过来并应用到本地数据库，Leader 服务器就将该 Follower 服务器加入到真正可用的 Follower 列表中。

那 ZAB 是如何处理那些需要被丢弃的提案 Proposal 呢？

ZXID 是一个 64 位的数字，其中低 32 位可看作是计数器，Leader 服务器每产生一个新的提案 Proposal 的时候，该计数器进行加 1 操作。而高 32 位表示 Leader 周期 epoch 的编号，每当选举一个新的 Leader 服务器，就会从该服务器本地的提案日志中最大 Proposal 的 ZXID 中解析出对应的 epoch 值，然后对其加 1 操作，这个值就作为新的 epoch 值，并将低 32 位初始化为 0 来开始生成新的 ZXID。

![img](../shortcuts/(null)-20220305163056035.(null))

基于这样的策略，当一个包含上一个 Leader 周期中尚未提交的提案 Proposal 的服务器启动时，以 Follower 角色加入集群中之后，Leader 服务器会根据自己服务器上最后被提交的 Proposal 来和 Follower 服务器的 Proposal 进行比对，比对结果就是 Leader 会要求 Follower 进行一个回退操作——回退到一个确实已经被集群中过半机器提交的最新的提案 Proposal（丢弃已经被跳过的提案）。

##### Master 选举实现细节

上文说过，新选举出来的 Leader 具有最大的 ZXID 的提案 Proposal，那这是怎么实现的呢？

ZAB 默认采用 TCP 版本的 FastLeaderElection 选举算法。在选举投票消息中包含了两个最基本的信息： SID 和 ZXID，分别表示被推举服务器的唯一标识（每台机器不一样）和提案 ID。假如投票信息为 （SID, ZXID）的形式。在第一次投票的时候，由于还无法检测集群其他机器的状态信息，因此每台机器都将自己作为被推举的对象来进行投票。每次对收到的投票，都是一个对投票信息（SID, ZXID）对比的过程，规则如下：

- **如果接收到的投票 ZXID 大于自己的 ZXID，就认可当前收到的投票，并再次将该投票发送出去。**

- **如果 ZXID 小于自己的 ZXID，那么就坚持的投票，不做任何变更。**

- **如果 ZXID 等于自己的 ZXID，再对比** **SID****，比自己大，就认可当前收到的投票，再将该投票发送出去；如果比自己小，那就坚持自己的投票，不做变更。**

经过第二次投票后，集群中每台机器都会再次收到其他机器的投票，然后开始统计，如果一台机器收到超过了半数的相同投票，那么这个投票对应的 SID 机器即为 Leader。

简单来说，通常哪台服务器上的数据越新，那么越有可能成为 Leader。原因很简单，数据越新，也就越能够保证数据的恢复。当然，如果集群中有几个服务器具有相同的 ZXID，那么 SID 较大的那台服务器成为 Leader。





## Raft

学习动画：[Raft](http://thesecretlivesofdata.com/raft/)

Raft 是一个非[拜占庭](https://bytedance.feishu.cn/docs/doccnvquUJSQ6BF3cYgVfbDgUqd#tiFNHB)的一致性算法，即所有通信是正确的而非伪造的。N 个结点的情况下可以最多容忍 (N−1)/2 个结点故障。

![img](../shortcuts/(null)-20220305163056123.(null))

Raft 正常工作时的流程如上图，也就是正常情况下日志复制的流程。Raft 中使用 **日志** 来记录所有操作，所有结点都有自己的日志列表来记录所有请求。算法将机器分成三种角色：Leader、Follower 和 Candidate。正常情况下只存在一个 Leader，其他均为 Follower，所有客户端都与 Leader 进行交互。

所有操作采用类似两阶段提交的方式，Leader 在收到来自客户端的请求后并不会执行，只是将其写入自己的日志列表中，然后将该操作发送给所有的 Follower。Follower 在收到请求后也只是写入自己的日志列表中然后回复 Leader，当有超过半数的结点写入后 Leader 才会提交该操作并返回给客户端，同时通知所有其他结点提交该操作。

通过这一流程保证了只要提交过后的操作一定在多数结点上留有记录（在日志列表中），从而保证了该数据不会丢失。

### 领导选举

在了解了算法的基本工作流程之后，就让我们开始解决其中会遇到的问题，首先就是 Leader 如何而来。

#### 初次选举

在算法刚开始时，所有结点都是 Follower，每个结点都会有一个定时器，每次收到来自 Leader 的信息就会更新该定时器。

![img](../shortcuts/(null)-20220305163056193.(null))

如果定时器超时，说明一段时间内没有收到 Leader 的消息，那么就可以认为 Leader 已死或者不存在，那么该结点就会转变成 Candidate，意思为准备竞争成为 Leader。

将自身 term 值 +1 ，成为 Candidate 后结点会向所有其他结点发送请求投票的请求（RequestVote），其他结点在收到请求后会判断是否可以投给他并返回结果。Candidate 如果收到了半数以上的投票就可以成为 Leader，成为之后会立即并在任期内定期发送一个心跳信息通知其他所有结点此时新的 Leader 信息，并用来重置定时器，避免其他结点再次成为 Candidate。

如果 Candidate 在一定时间内没有获得足够的投票，那么就会进行一轮新的选举，直到其成为 Leader,或者其他结点成为了新的 Leader，自己变成 Follower。

#### 再次选举

再次选举会在两种情况下发生。

![img](../shortcuts/(null)-20220305163056207.(null))

第一种情况是 Leader 下线，此时所有其他结点的计时器不会被重置，直到一个结点成为了 Candidate，和上述一样开始一轮新的选举选出一个新的 Leader。

![img](../shortcuts/(null)-20220305163056391.(null))

第二种情况是某一 Follower 结点与 Leader 间通信发生问题，导致发生了分区，这时没有 Leader 的那个分区就会进行一次选举。这种情况下，因为要求获得多数的投票才可以成为 Leader，因此只有拥有多数结点的分区可以正常工作。而对于少数结点的分区，即使仍存在 Leader，但由于写入日志的结点数量不可能超过半数因此不可能提交操作。这解释了为何 Raft 至多容忍 (N−1)/2 个结点故障。

![img](../shortcuts/(null)-20220305163056198.(null))

这解释了每个结点会如何在三个状态间发生变化。

#### 任期 Term

Leader 的选举引出了一个新的概念——**任期**（Term）。

![img](../shortcuts/(null)-20220305163056214.(null))

每一个任期以一次选举作为起点，所以当一个结点成为 Candidate 并向其他结点请求投票时，会将自己的 Term 加 1，表明新一轮的开始以及旧 Leader 的任期结束。所有节点在收到比自己更大的 Term 之后就会更新自己的 Term 并转成 Follower，而收到过时的消息则拒绝该请求。

在一次成功选举完成后，Leader 会负责管理所有结点直至任期结束。如果没有产生新的 Leader 就会开始一轮新的 Term。任期在 Raft 起到了类似时钟的功能，用于检测信息是否过期。

#### 投票限制

在投票时候，所有服务器采用先来先得的原则，在一个任期内只可以投票给一个结点，得到超过半数的投票才可成为 Leader，从而保证了一个任期内只会有一个 Leader 产生（**Election Safety**）。

在 Raft 中日志只有从 Leader 到 Follower 这一流向，所以需要保证 Leader 的日志必须正确，即必须拥有所有已在多数节点上存在的日志，这一步骤由投票来限制。

![img](../shortcuts/(null)-20220305163056219.(null))

投票由一个称为 RequestVote 的 RPC 调用进行，请求中除了有 Candidate 自己的 term 和 id 之外，还要带有自己最后一个日志条目的 index 和 term。接收者收到后首先会判断请求的 term 是否更大，不是则说明是旧消息，拒绝该请求。如果任期更大则开始判断日志是否更加新。日志 Term 越大则越新，相同那么 index 较大的认为是更加新的日志。接收者只会投票给拥有相同或者更加新的日志的 Candidate。

由于只有日志在被多数结点复制之后才会被提交并返回，所以如果一个 Candidate 并不拥有最新的已被复制的日志，那么他不可能获得多数票，从而保证了 Leader 一定具有所有已被多数拥有的日志（**Leader Completeness**），在后续同步时会将其同步给所有结点。

#### 定时器时间

定时器时间的设定实际上也会影响到算法性能甚至是正确性。试想一下这样一个场景，Leader 下线，有两个结点同时成为 Candidate，然后由于网络结构等原因，每个结点都获得了一半的投票，因此无人成为 Leader 进入了下一轮。然而在下一轮由于这两个结点同时结束，又同时成为了 Candidate，再次重复了之前的这一流程，那么算法就无法正常工作。

为了解决这一问题，Raft 采用了一个十分“艺术”的解决方法，随机定时器长短（例如 150-300ms）。通过这一方法避免了两个结点同时成为 Candidate，即使发生了也能快速恢复。这一长短必须长于 Leader 的心跳间隔，否则在正常情况下也会有 Candidate 出现导致算法无法正常工作。

### 日志复制

在之前的[工作流程](https://zinglix.xyz/2020/06/25/raft/#工作流程)章节中已经描述了日志是如何被复制到其他结点上的，但实际中还会发生结点下线，从而产生不一致的情况的发生，也是这一章我们将要讨论的内容。

#### 前提

Raft 保证了如下几点：

- Leader 绝不会覆盖或删除自己的日志，只会追加 （**Leader Append-Only**）

- 如果两个日志的 index 和 term 相同，那么这两个日志相同 （**Log Matching**）

- 如果两个日志相同，那么他们之前的日志均相同

第一点主要是因为选举时的限制，根据 Leader Completeness，成为 Leader 的结点里的日志一定拥有所有已被多数节点拥有的日志条目，所以先前的日志条目很可能已经被提交，因此不可以删除之前的日志。

第二点主要是因为一个任期内只可能出现一个 Leader，而 Leader 只会为一个 index 创建一个日志条目，而且一旦写入就不会修改，因此保证了日志的唯一性。

第三点是因为在写入日志时会检查前一个日志是否一致。换言之就是，如果写入了一条日志，那么前一个日志条目也一定一致，从而递归的保证了前面的所有日志都一致。从而也保证了当一个日志被提交之后，所有结点在该 index 上提交的内容是一样的（**State Machine Safety**）。

#### 日志同步

接下来我们就可以看到 Raft 实际中是如何做到日志同步的。这一过程由一个称为 AppendEntries 的 RPC 调用实现，Leader 会给每个 Follower 发送该 RPC 以追加日志，请求中除了当前任期 term、Leader 的 id 和已提交的日志 index，还有将要追加的日志列表（空则成为心跳包），前一个日志的 index 和 term。

![img](../shortcuts/(null)-20220305163056311.(null))

当接收到该请求后，会先检查 term，如果请求中的 term 比自己的小说明已过期，拒绝请求。之后会对比先前日志的 index 和 term，如果一致，那么由前提可知前面的日志均相同，那么就可以从此处更新日志，将请求中的所有日志写入自己的日志列表中，否则返回 false。如果发生 index 相同但 term 不同则清空后续所有的日志，以 Leader 为准。最后检查已提交的日志 index，对可提交的日志进行提交操作。

对于 Leader 来说会维护 nextIndex[] 和 matchIndex[] 两个数组，分别记录了每个 Follower 下一个将要发送的日志 index 和已经匹配上的日志 index。每次成为 Leader 都会初始化这两个数组，前者初始化为 Leader 最后一条日志的 index 加 1，后者初始化为 0。每次发送 RPC 时会发送 nextIndex[i] 及之后的日志，成功则更新两个数组，否则减少 nextIndex[i] 的值重试，重复这一过程直至成功。

> *这里减少 nextIndex 的值有不同的策略，可以每次减一，也可以减一个较大的值，或者是跨任期减少，用于快速找到和该结点相匹配的日志条目。实际中还有可能会定期存储日志，所以当前日志列表中并不会太大，可以完整打包发给对方，这一做法比较适合新加入集群的结点。*

#### 日志提交

只要日志在多数结点上存在，那么 Leader 就可以提交该操作。但是 Raft 额外限制了 Leader 只对自己任期内的日志条目适用该规则，先前任期的条目只能由当前任期的提交而间接被提交。

![img](../shortcuts/(null)-20220305163056308.(null))

例如论文中图 8 这一 corner case。一开始如 (a) 所示，之后 S1 下线，(b) 中 S5 从 S3 和 S4 处获得了投票成为了 Leader 并收到了一条来自客户端的消息，之后 S5 下线。(c) 中 S1 恢复并成为了 Leader，并且将日志复制给了多数结点，之后进行了一个致命操作，将 index 为 2 的日志提交了，然后 S1 下线。(d) 中 S5 恢复，并从 S2、S3、S4 处获得了足够投票，然后将已提交的 index 为 2 的日志覆盖了。

为了解决这个问题，Raft 只允许提交自己任期内的日志，从而日志 2 只能像 (e) 中由于日志 3 同步而被间接提交，避免了 Follower 中由于缺少新任期的日志，使得 S5 能够继续成为 Leader。





## Paxos vs ZAB vs Raft

|          | **[Paxos](https://bytedance.feishu.cn/docs/doccnvquUJSQ6BF3cYgVfbDgUqd#dvx13b)** | **[ZAB](https://bytedance.feishu.cn/docs/doccnvquUJSQ6BF3cYgVfbDgUqd#91Dnmv)** | **[Raft](https://bytedance.feishu.cn/docs/doccnvquUJSQ6BF3cYgVfbDgUqd#6mx5aG)** |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 角色     | `Proposer提案者`、`Acceptor表决者`、`Learner学习者`          | `Leader领导者`、`Follower跟随者`、`Observer观察者`           | `Leader`、`Follower` 和 `Candidate`                          |
| 重要概念 | 提案。最终要达成一致的 value 就在提案里。只要 Proposer 发的提案被半数以上的 Acceptor 接受，Proposer 就认为该提案里的 value 被选定了。 | ZAB 参考了一些 Paxos 设计思想，比如： leader 向 followers 提出提案(proposal) leader 需要在达到法定数量(半数以上)的 followers 确认之后才会进行 commit 每一个 proposal 都有一个纪元(epoch)号 | 如果定时器超时，那么就可以认为 Leader 已死或者不存在，结点就会转变成 Candidate。 将自身 term 值 +1 ，成为 Candidate 后结点会向所有其他结点发送请求投票的请求，其他结点在收到请求后会判断是否可以投给他并返回结果。Candidate 如果收到了半数以上的投票就可以成为 Leader，成为之后会立即并在任期内定期发送一个心跳信息通知其他所有结点此时新的 Leader 信息，并用来重置定时器，避免其他结点再次成为 Candidate。 |



# 扩展

## 拜占庭将军问题

> 拜占庭将军问题是一个协议问题，拜占庭帝国军队的将军们必须全体一致的决定是否攻击某一支敌军。问题是这些将军在地理上是分隔开来的，并且将军中存在叛徒。叛徒可以任意行动以达到以下目标：欺骗某些将军采取进攻行动；促成一个不是所有将军都同意的决定，如当将军们不希望进攻时促成进攻行动；或者迷惑某些将军，使他们无法做出决定。如果叛徒达到了这些目的之一，则任何攻击行动的结果都是注定要失败的，只有完全达成一致的努力才能获得胜利。

这一问题是一种对现实世界的模型化，尤指网络当中由于软硬件错误、网络阻塞及恶意攻击导致的各种未知行为。

拜占庭将军问题提出后，有很多的算法被提出用于解决这个问题。这类算法统称拜占庭容错算法（BFT: Byzantine Fault Tolerance）。本质上来说，拜占庭容错方案就是少数服从多数。

拜占庭系统目前普遍采用的假设条件包括:

- [拜占庭节点](https://www.zhihu.com/search?q=拜占庭节点&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType":"article","sourceId":217827966})的行为可以是任意的，拜占庭节点之间可以共谋；

- 节点之间的错误是不相关的；

- 节点之间通过异步网络连接，网络中的消息可能丢失、乱序、延时到达；

- 服务器之间传递的信息, 第三方可以知晓 ,但是不能篡改、伪造信息的内容和验证信息的完整性；

### BFT 共识机制

在 BFT 共识机制中，网络中节点的数量和身份必须是提前确定好的。且每一次节点的进出都需要对网络进行初始化，故其无法像 PoW 共识机制那样任何人都可以随时加入/退出挖矿。另外，由于节点间基于消息传递达成共识，因此采用BFT算法的网络无法承载大量的节点，业内普遍认为100个节点是BFT算法的上限。所以BFT算法无法直接用于公有链，而更多的应用于私有链和联盟链。业内大名鼎鼎的联盟链Hyperledger fabric v0.6采用的是PBFT，v1.0又推出PBFT的改进版本SBFT。后续又有相当多的人对其进行了改进，力求提高其扩展性。但往往都是基于对网络环境的理想假设，以省去部分共识阶段，实现更高的[节点承载量](https://www.zhihu.com/search?q=节点承载量&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType":"article","sourceId":217827966})。

在可信环境下共识算法一般使用传统的分布式一致算法 PAXOS 或者 RAFT。

![img](../shortcuts/(null)-20220305163056329.(null))

## PBFT

参考 [实用拜占庭容错机制理解](https://zhuanlan.zhihu.com/p/217827966)

**为什么 PBFT 算法最大容错节点数量 f 是 (N-1)/3**

假定系统节点总数是N，作恶节点数为f，那么剩下的正确节点数为N - f，意味着只要收到N - f个消息且N - f > f就能做出决定，但还要注意收到的 N - f 个消息有可能有 f 个是由作恶节点冒充的（或因网络延迟导致 f 个[恶意节点](https://www.zhihu.com/search?q=恶意节点&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType":"article","sourceId":217827966})的消息先被收到），那么正确的消息就是 N - f - f 个，为了多数一致，正确消息必须占多数，也就是N - f - f > f ，所以N最少是 3f + 1 个。

# 参考

[In search of an Understandable Consensus Algorithm (Extended Version) | Raft](https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf)

[wiki | 分布式计算](https://zh.wikipedia.org/wiki/分布式计算)

[「分布式一致性协议」从2PC、3PC、Paxos到 ZAB](https://xie.infoq.cn/article/0b443ad92ada320b06f669dba)

[谈谈分布式系统的CAP理论](https://zhuanlan.zhihu.com/p/33999708)

[轻松理解CAP理论](https://zhuanlan.zhihu.com/p/50990721)

[分布式系统的一致性协议之 2PC 和 3PC | Matt's Blog](https://matt33.com/2018/07/08/distribute-system-consistency-protocol)

 [2PC到3PC到Paxos到Raft到ISR](https://segmentfault.com/a/1190000004474543)

[ZooKeeper 一致性协议 ZAB 原理分析! - 掘金](https://juejin.cn/post/6844903806723964936)

[实用拜占庭容错机制理解](https://zhuanlan.zhihu.com/p/217827966)

[「图解Raft」让一致性算法变得更简单](https://zinglix.xyz/2020/06/25/raft/)